{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING LIBRARIES\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 0) DEVICE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 1) ENVIRONMENT\n",
    "class DroneDispatchEnv:\n",
    "    def __init__(self, H, M, N, L,\n",
    "                 zeta, epsilon_min,\n",
    "                 breakpoints_slow, breakpoints_fast,\n",
    "                 alpha, beta, cap_i,\n",
    "                 lambda_h_t, sigma_h,\n",
    "                 distances, speeds,\n",
    "                 eta, psi, sell_price):\n",
    "        self.H, self.M, self.N, self.L = H, M, N, L\n",
    "        self.zeta, self.epsilon_min = zeta, epsilon_min\n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        self.cap_i = cap_i.copy()\n",
    "        self.lambda_h_t = lambda_h_t.copy()\n",
    "        self.sigma_h = sigma_h.copy()\n",
    "        self.distances = distances.copy()\n",
    "        self.speeds = speeds.copy()\n",
    "        self.eta, self.psi = eta, psi\n",
    "        self.sell_price = sell_price\n",
    "\n",
    "        # normalization constants\n",
    "        self.max_demand = self.lambda_h_t.max()\n",
    "        self.reward_scale = float(self.M * self.sigma_h.max() * self.cap_i.max())\n",
    "\n",
    "        self.bp_slow = sorted(breakpoints_slow, key=lambda x: x[0])\n",
    "        self.bp_fast = sorted(breakpoints_fast, key=lambda x: x[0])\n",
    "        self._build_piecewise()\n",
    "        self.action_dim = 3 + H\n",
    "        self.reset()\n",
    "\n",
    "    def _build_piecewise(self):\n",
    "        ts, ys = zip(*self.bp_slow)\n",
    "        self.t_slow_bp = np.array(ts); self.s_slow_bp = np.array(ys)\n",
    "        self.sl_slow = (self.s_slow_bp[1:]-self.s_slow_bp[:-1])/(self.t_slow_bp[1:]-self.t_slow_bp[:-1])\n",
    "        tf, yf = zip(*self.bp_fast)\n",
    "        self.t_fast_bp = np.array(tf); self.s_fast_bp = np.array(yf)\n",
    "        self.sl_fast = (self.s_fast_bp[1:]-self.s_fast_bp[:-1])/(self.t_fast_bp[1:]-self.t_fast_bp[:-1])\n",
    "\n",
    "    def t_slow_from_soc(self, soc):\n",
    "        for i in range(len(self.sl_slow)):\n",
    "            if self.s_slow_bp[i] <= soc <= self.s_slow_bp[i+1]:\n",
    "                return self.t_slow_bp[i] + (soc-self.s_slow_bp[i])/self.sl_slow[i]\n",
    "        return self.t_slow_bp[-1]\n",
    "\n",
    "    def t_fast_from_soc(self, soc):\n",
    "        for i in range(len(self.sl_fast)):\n",
    "            if self.s_fast_bp[i] <= soc <= self.s_fast_bp[i+1]:\n",
    "                return self.t_fast_bp[i] + (soc-self.s_fast_bp[i])/self.sl_fast[i]\n",
    "        return self.t_fast_bp[-1]\n",
    "\n",
    "    def soc_slow(self, t):\n",
    "        for i in range(len(self.sl_slow)):\n",
    "            if self.t_slow_bp[i] <= t <= self.t_slow_bp[i+1]:\n",
    "                return self.s_slow_bp[i] + self.sl_slow[i]*(t-self.t_slow_bp[i])\n",
    "        return self.s_slow_bp[-1]\n",
    "\n",
    "    def soc_fast(self, t):\n",
    "        for i in range(len(self.sl_fast)):\n",
    "            if self.t_fast_bp[i] <= t <= self.t_fast_bp[i+1]:\n",
    "                return self.s_fast_bp[i] + self.sl_fast[i]*(t-self.t_fast_bp[i])\n",
    "        return self.s_fast_bp[-1]\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.soc = np.full(self.M, 100.0)\n",
    "        self.mode = np.zeros(self.M, dtype=int)\n",
    "        self.remain = np.zeros(self.M, dtype=int)\n",
    "        self.target = -np.ones(self.M, dtype=int)\n",
    "        self.demand = np.random.poisson(self.lambda_h_t[:,0])\n",
    "        self.dispatch_demand = np.zeros(self.M)\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # time ∈ [0,1], soc ∈ [0,1], mode ∈ [0,1], remain ∈ [0,1], demand ∈ [0,1]\n",
    "        t_norm    = self.t / self.N\n",
    "        soc_norm  = (np.floor(self.soc/self.zeta)*self.zeta)/100.0\n",
    "        mode_norm = self.mode.astype(float)/3.0\n",
    "        rem_norm  = self.remain.astype(float)/self.N\n",
    "        dem_norm  = self.demand.astype(float)/4\n",
    "        return np.concatenate([[t_norm], soc_norm, mode_norm, rem_norm, dem_norm])\n",
    "\n",
    "    def _get_raw_state(self):\n",
    "        \"\"\"Get unnormalized state for logging purposes\"\"\"\n",
    "        t_raw = self.t\n",
    "        soc_raw = np.floor(self.soc/self.zeta)*self.zeta  # Discretized but not normalized\n",
    "        mode_raw = self.mode.copy()\n",
    "        rem_raw = self.remain.copy()\n",
    "        dem_raw = self.demand.copy()\n",
    "        return {\n",
    "            'timestep': t_raw,\n",
    "            'soc': soc_raw.tolist(),\n",
    "            'mode': mode_raw.tolist(), \n",
    "            'remain': rem_raw.tolist(),\n",
    "            'demand': dem_raw.tolist()\n",
    "        }\n",
    "\n",
    "    def _calculate_terminal_demand_potential_with_details(self, socs):\n",
    "        \"\"\"\n",
    "        Returns both terminal reward and complete calculation details for logging\n",
    "        Sequential assignment terminal reward with demand depletion\n",
    "        \"\"\"\n",
    "        # Sample terminal demands from Poisson distribution\n",
    "        terminal_demands = {}\n",
    "        for hub in range(self.H):\n",
    "            lambda_h_avg = np.mean(self.lambda_h_t[hub, :])\n",
    "            terminal_demands[hub] = np.random.poisson(lambda_h_avg)\n",
    "        \n",
    "        remaining_demand = terminal_demands.copy()\n",
    "        total_potential = 0\n",
    "        assignment_details = []  # Track assignment process for logging\n",
    "        \n",
    "        # Sequential assignment from drone 0 to M-1 (matching mathematical formulation)\n",
    "        for agent_id in range(self.M):\n",
    "            agent_max_potential = 0\n",
    "            best_hub = -1\n",
    "            best_serviceable = 0\n",
    "            \n",
    "            # Find the best hub for this agent given remaining demands\n",
    "            for hub in range(self.H):\n",
    "                if remaining_demand[hub] > 0:\n",
    "                    # Calculate actual payload for energy calculation\n",
    "                    actual_payload = min(remaining_demand[hub], self.cap_i[agent_id])\n",
    "                    \n",
    "                    # Check if dispatch is possible with actual payload\n",
    "                    can_dispatch = self._can_dispatch_terminal(agent_id, hub, socs[agent_id], remaining_demand[hub])\n",
    "                    \n",
    "                    if can_dispatch:\n",
    "                        serviceable = actual_payload\n",
    "                        potential_reward = serviceable * self.sigma_h[hub]\n",
    "                        \n",
    "                        if potential_reward > agent_max_potential:\n",
    "                            agent_max_potential = potential_reward\n",
    "                            best_hub = hub\n",
    "                            best_serviceable = serviceable\n",
    "            \n",
    "            # Track assignment details for logging\n",
    "            assignment_details.append({\n",
    "                'agent_id': agent_id,\n",
    "                'soc': float(socs[agent_id]),\n",
    "                'best_hub': int(best_hub) if best_hub >= 0 else -1,\n",
    "                'reward': float(agent_max_potential),\n",
    "                'serviceable': int(best_serviceable) if best_hub >= 0 else 0\n",
    "            })\n",
    "            \n",
    "            # Update remaining demand based on agent's best choice\n",
    "            if best_hub >= 0:\n",
    "                remaining_demand[best_hub] = max(0, remaining_demand[best_hub] - best_serviceable)\n",
    "            \n",
    "            total_potential += agent_max_potential\n",
    "        \n",
    "        # Return complete information for logging\n",
    "        terminal_info = {\n",
    "            'sampled_demands': terminal_demands,\n",
    "            'assignment_details': assignment_details,\n",
    "            'final_remaining_demands': remaining_demand,\n",
    "            'lambda_averages': [np.mean(self.lambda_h_t[hub, :]) for hub in range(self.H)]\n",
    "        }\n",
    "        \n",
    "        return total_potential, terminal_info\n",
    "\n",
    "    def _can_dispatch_terminal(self, agent_id, hub, current_soc, available_demand):\n",
    "        \"\"\"Check if agent can dispatch to hub with given SOC and actual payload (CORRECTED)\"\"\"\n",
    "        actual_payload = min(available_demand, self.cap_i[agent_id])  # Consider actual payload\n",
    "        flight_time = self.distances[hub] / self.speeds[agent_id]  # FLIGHT TIME, not distance!\n",
    "        energy_needed = (self.alpha * actual_payload + 2 * self.beta) * flight_time  # Multiply by TIME\n",
    "        threshold = self.zeta * math.ceil((energy_needed + self.epsilon_min) / self.zeta)\n",
    "        return current_soc >= threshold\n",
    "\n",
    "    def step(self, actions):\n",
    "        reward = 0.0\n",
    "        rem_dem = self.demand.copy()\n",
    "        \n",
    "        # reset dispatch_demand for drones NOT in flight\n",
    "        for i in range(self.M):\n",
    "            if self.mode[i] != 3:  # Not in flight\n",
    "                self.dispatch_demand[i] = 0\n",
    "\n",
    "        # fleet-wide action effects\n",
    "        for i,a in enumerate(actions):\n",
    "            if self.mode[i] in (0,1,2):\n",
    "                if a==0:\n",
    "                    self.mode[i],self.remain[i]=0,0\n",
    "                elif a==1:\n",
    "                    t0=self.t_slow_from_soc(self.soc[i])\n",
    "                    t1=min(t0+self.L,self.t_slow_bp[-1])\n",
    "                    self.soc[i]=min(100,self.soc_slow(t1))\n",
    "                    self.mode[i],self.remain[i]=1,0\n",
    "                    reward-=self.eta\n",
    "                elif a==2:\n",
    "                    t0=self.t_fast_from_soc(self.soc[i])\n",
    "                    t1=min(t0+self.L,self.t_fast_bp[-1])\n",
    "                    self.soc[i]=min(100,self.soc_fast(t1))\n",
    "                    self.mode[i],self.remain[i]=2,0\n",
    "                    reward-=self.psi\n",
    "                else:\n",
    "                    h=a-3\n",
    "                    if self._can_dispatch(i,h):\n",
    "                        pay=min(rem_dem[h],self.cap_i[i])\n",
    "                        rem_dem[h]-=pay\n",
    "                        reward+=self.sigma_h[h]*pay\n",
    "                        self.dispatch_demand[i]=pay  # Now preserved for in-flight drones\n",
    "                        T1=self.distances[h]/self.speeds[i]\n",
    "                        k1=math.ceil(T1/self.L)\n",
    "                        self.mode[i],self.remain[i],self.target[i]=3,2*k1,h\n",
    "\n",
    "        # in-flight discharge\n",
    "        for i in range(self.M):\n",
    "            if self.mode[i]==3:\n",
    "                h=int(self.target[i])\n",
    "                T1=self.distances[h]/self.speeds[i]\n",
    "                k1=math.ceil(T1/self.L); k=2*k1\n",
    "                q=k-self.remain[i]+1\n",
    "                # uses correct payload from dispatch_demand[i]\n",
    "                c=(self.alpha*self.dispatch_demand[i]+self.beta)*self.L if q<=k1 else self.beta*self.L\n",
    "                new_soc=max(0,self.soc[i]-c)\n",
    "                self.soc[i]=self.zeta*math.floor(new_soc/self.zeta)\n",
    "                self.remain[i]-=1\n",
    "                if self.remain[i]<=0:\n",
    "                    self.mode[i],self.remain[i],self.target[i]=0,0,-1\n",
    "                    self.dispatch_demand[i]=0  # Reset when mission complete\n",
    "\n",
    "        self.t+=1\n",
    "        done=(self.t>=self.N)  # When t reaches N, episode is done\n",
    "        \n",
    "        if not done:\n",
    "            self.demand=np.random.poisson(self.lambda_h_t[:,self.t]) if self.t<self.lambda_h_t.shape[1] else np.zeros(self.H,dtype=int)\n",
    "\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "    def _can_dispatch(self,i,h):\n",
    "        pay=min((self.dispatch_demand[i] if self.mode[i]==3 else self.demand[h]),self.cap_i[i])\n",
    "        flight=self.distances[h]/self.speeds[i]\n",
    "        need=(self.alpha*pay+2*self.beta)*flight\n",
    "        thresh=self.zeta*math.ceil((need+self.epsilon_min)/self.zeta)\n",
    "        return self.soc[i]>=thresh\n",
    "\n",
    "\n",
    "# 2) FEASIBLE ACTIONS\n",
    "def get_feasible_actions(env,i,rem):\n",
    "    feasible=[0]\n",
    "    if env.mode[i]!=3 and env.soc[i]<100: feasible+=[1,2]\n",
    "    if env.mode[i]!=3:\n",
    "        for h in range(env.H):\n",
    "            if rem[h]>0 and env._can_dispatch(i,h):\n",
    "                feasible.append(3+h)\n",
    "    return feasible\n",
    "\n",
    "\n",
    "# 3) NETWORKS FOR COMA\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class COMACritic(nn.Module):\n",
    "    \"\"\"\n",
    "    COMA Critic: Q(s, u^-i, i) -> Q-values for ALL actions of agent i\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, M, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "        # Input: state + other agents' actions (one-hot) + agent_id\n",
    "        input_dim = state_dim + (M-1) * action_dim + 1\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)  # Output Q-values for ALL actions\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, other_actions, agent_id):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: (batch_size, state_dim)\n",
    "            other_actions: (batch_size, (M-1)*action_dim) - other agents' actions (one-hot)\n",
    "            agent_id: (batch_size, 1) - which agent this is for\n",
    "        Returns:\n",
    "            Q-values: (batch_size, action_dim) - Q(s, u^-i, i) for ALL u^i\n",
    "        \"\"\"\n",
    "        agent_id_normalized = agent_id.float() / self.M\n",
    "        x = torch.cat([state, other_actions, agent_id_normalized], dim=1)\n",
    "        return self.net(x)  # Shape: (batch_size, action_dim)\n",
    "\n",
    "\n",
    "# 4) RUN EPISODE WITH DETAILED LOGGING\n",
    "def run_episode(env, actors):\n",
    "    state = env.reset()\n",
    "    traj = {\n",
    "        'states': [], 'local_obs': [], 'actions': [], 'rewards': [], \n",
    "        'feasible_masks': [], 'log': []\n",
    "    }\n",
    "    \n",
    "    # Detailed episode log\n",
    "    episode_log = []\n",
    "    \n",
    "    while True:\n",
    "        # Check if episode is done BEFORE taking actions\n",
    "        if env.t >= env.N - 1:\n",
    "            # Calculate terminal reward\n",
    "            soc_disc = np.floor(env.soc/env.zeta)*env.zeta\n",
    "            terminal_reward, terminal_info = env._calculate_terminal_demand_potential_with_details(soc_disc)\n",
    "            \n",
    "            # Log terminal step (no action, no next_state)\n",
    "            episode_log.append({\n",
    "                'step': env.t,\n",
    "                'state': env._get_raw_state(),\n",
    "                'action': None,  # No action in terminal state\n",
    "                'next_state': None,  # No next state\n",
    "                'reward': float(terminal_reward),\n",
    "                'terminal': True,\n",
    "                'terminal_info': terminal_info\n",
    "            })\n",
    "            \n",
    "            traj['rewards'].append(terminal_reward)\n",
    "            break\n",
    "        \n",
    "        traj['states'].append(state.copy())\n",
    "        \n",
    "        # Store current state for logging\n",
    "        current_raw_state = env._get_raw_state()\n",
    "        \n",
    "        # Local observations for each agent\n",
    "        local_feats = [\n",
    "            np.concatenate([\n",
    "                [state[0]], [state[1+i]], [state[1+env.M+i]],\n",
    "                [state[1+2*env.M+i]], state[-env.H:]\n",
    "            ]) for i in range(env.M)\n",
    "        ]\n",
    "        traj['local_obs'].append(local_feats)\n",
    "        \n",
    "        rem = env.demand.copy()\n",
    "        acts = []\n",
    "        feasible_masks = []\n",
    "        \n",
    "        for i, actor in enumerate(actors):\n",
    "            feas = get_feasible_actions(env, i, rem)\n",
    "            \n",
    "            # Store feasible mask\n",
    "            mask = torch.zeros(env.action_dim, device=device)\n",
    "            mask[feas] = 1.0\n",
    "            feasible_masks.append(mask)\n",
    "            \n",
    "            # Get action from policy\n",
    "            obs = torch.FloatTensor(local_feats[i]).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = actor(obs).squeeze(0)\n",
    "            \n",
    "            # Apply feasible mask\n",
    "            masked_logits = torch.full_like(logits, -1e9)\n",
    "            masked_logits[feas] = logits[feas]\n",
    "            probs = torch.softmax(masked_logits, dim=-1)\n",
    "            \n",
    "            # Sample from policy\n",
    "            a = int(torch.multinomial(probs, 1).item())\n",
    "            \n",
    "            acts.append(a)\n",
    "            if a >= 3:\n",
    "                rem[a-3] = max(0, rem[a-3] - env.cap_i[i])\n",
    "\n",
    "        traj['feasible_masks'].append(feasible_masks)\n",
    "        traj['actions'].append(acts.copy())\n",
    "        \n",
    "        # Step environment\n",
    "        next_state, reward, done = env.step(acts)\n",
    "        traj['rewards'].append(reward)\n",
    "        \n",
    "        # Log this step\n",
    "        episode_log.append({\n",
    "            'step': env.t - 1,  # env.t was incremented in step()\n",
    "            'state': current_raw_state,\n",
    "            'action': acts.copy(),\n",
    "            'next_state': env._get_raw_state(),\n",
    "            'reward': float(reward),\n",
    "            'terminal': False\n",
    "        })\n",
    "        \n",
    "        traj['log'].append({\n",
    "            'state': state.copy(), 'action': acts.copy(), \n",
    "            'reward': reward, 'next_state': next_state.copy(), 'done': done\n",
    "        })\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "    return traj, episode_log\n",
    "\n",
    "\n",
    "# 5) ON-POLICY COMA TRAINING WITH DETAILED LOGGING\n",
    "def train_coma_standard(env, episodes=8000, gamma=0.95, lr_actor=3e-4, lr_critic=1e-3, entropy_coef=0.01):\n",
    "    \n",
    "    state_dim = len(env.reset())\n",
    "    M, H = env.M, env.H\n",
    "    action_dim = 3 + H\n",
    "    obs_dim = 4 + H\n",
    "    \n",
    "    # Initialize networks\n",
    "    actors = [Actor(obs_dim, action_dim, hidden_dim=128).to(device) for _ in range(M)]\n",
    "    critic = COMACritic(state_dim, M, action_dim, hidden_dim=256).to(device)\n",
    "    \n",
    "    # Optimizers\n",
    "    actor_optimizers = [optim.Adam(actor.parameters(), lr=lr_actor, weight_decay=1e-5) \n",
    "                       for actor in actors]\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=lr_critic, weight_decay=1e-4)\n",
    "    \n",
    "    # Training logs\n",
    "    all_returns, critic_losses, actor_losses = [], [], []\n",
    "    \n",
    "    # Detailed episode logs\n",
    "    detailed_episode_logs = {}\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Run episode with detailed logging\n",
    "        traj, episode_log = run_episode(env, actors)\n",
    "        \n",
    "        # Store detailed episode log\n",
    "        detailed_episode_logs[episode] = episode_log\n",
    "        \n",
    "        T = len(traj['actions'])  # Number of action timesteps\n",
    "        \n",
    "        # Calculate returns for this episode only (Monte Carlo)\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for t in reversed(range(T + 1)):  # Include terminal reward\n",
    "            G = traj['rewards'][t] + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        total_reward = returns[0] if returns else 0\n",
    "        all_returns.append(total_reward)\n",
    "        \n",
    "        # ON-POLICY NETWORK UPDATES\n",
    "        episode_critic_loss = 0.0\n",
    "        episode_actor_loss = 0.0\n",
    "        \n",
    "        if T > 0:  # Make sure we have some transitions\n",
    "            # Convert episode data to tensors\n",
    "            states = torch.from_numpy(np.stack(traj['states'], 0)).float().to(device)  # (T, state_dim)\n",
    "            actions = torch.LongTensor(traj['actions']).to(device)  # (T, M)\n",
    "            returns_tensor = torch.FloatTensor(returns[:-1]).to(device)  # (T,) - exclude terminal\n",
    "            \n",
    "            # CRITIC UPDATE\n",
    "            critic_loss = 0.0\n",
    "            \n",
    "            for i in range(M):\n",
    "                # Prepare other agents' actions (one-hot)\n",
    "                other_actions_onehot = torch.zeros(T, (M-1) * action_dim, device=device)\n",
    "                for t in range(T):\n",
    "                    other_idx = 0\n",
    "                    for j in range(M):\n",
    "                        if j != i:\n",
    "                            other_actions_onehot[t, other_idx * action_dim + actions[t, j]] = 1.0\n",
    "                            other_idx += 1\n",
    "                \n",
    "                agent_ids = torch.full((T, 1), i, device=device)\n",
    "                \n",
    "                # Get Q-values for ALL actions of agent i\n",
    "                q_values_all = critic(states, other_actions_onehot, agent_ids)  # (T, action_dim)\n",
    "                \n",
    "                # Extract Q-value for the action that was actually taken\n",
    "                q_values_taken = q_values_all.gather(1, actions[:, i].unsqueeze(1)).squeeze(1)\n",
    "                \n",
    "                # Critic loss for agent i\n",
    "                critic_loss_i = F.mse_loss(q_values_taken, returns_tensor)\n",
    "                critic_loss += critic_loss_i\n",
    "            \n",
    "            critic_loss = critic_loss / M\n",
    "            episode_critic_loss = critic_loss.item()\n",
    "            \n",
    "            # Update critic\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=10.0)\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            # ACTOR UPDATES\n",
    "            total_actor_loss = 0.0\n",
    "            \n",
    "            for i in range(M):\n",
    "                agent_actor_loss = 0.0\n",
    "                valid_samples = 0\n",
    "                \n",
    "                for t in range(T):\n",
    "                    feasible_mask = traj['feasible_masks'][t][i]\n",
    "                    feasible_actions = torch.where(feasible_mask > 0)[0].cpu().numpy()\n",
    "                    \n",
    "                    if len(feasible_actions) <= 1:\n",
    "                        continue\n",
    "                    \n",
    "                    # Prepare other agents' actions\n",
    "                    other_actions_onehot = torch.zeros((M-1) * action_dim, device=device)\n",
    "                    other_idx = 0\n",
    "                    for j in range(M):\n",
    "                        if j != i:\n",
    "                            other_actions_onehot[other_idx * action_dim + actions[t, j]] = 1.0\n",
    "                            other_idx += 1\n",
    "                    \n",
    "                    state_t = states[t:t+1]\n",
    "                    other_actions_t = other_actions_onehot.unsqueeze(0)\n",
    "                    agent_id_t = torch.tensor([[i]], device=device)\n",
    "                    \n",
    "                    # COMA ADVANTAGE COMPUTATION\n",
    "                    with torch.no_grad():\n",
    "                        # Get Q-values for ALL actions of agent i\n",
    "                        q_values_all = critic(state_t, other_actions_t, agent_id_t).squeeze(0)  # (action_dim,)\n",
    "                        \n",
    "                        # Get current policy probabilities\n",
    "                        obs_i = torch.FloatTensor(traj['local_obs'][t][i]).unsqueeze(0).to(device)\n",
    "                        logits_i = actors[i](obs_i).squeeze(0)\n",
    "                        \n",
    "                        # Apply feasible mask\n",
    "                        masked_logits = torch.full_like(logits_i, -1e9)\n",
    "                        masked_logits[feasible_actions] = logits_i[feasible_actions]\n",
    "                        policy_probs = F.softmax(masked_logits, dim=-1)\n",
    "                        \n",
    "                        # Compute counterfactual baseline\n",
    "                        baseline = torch.sum(policy_probs * q_values_all).item()\n",
    "                        \n",
    "                        # Q-value for the action that was actually taken\n",
    "                        action_taken = actions[t, i]\n",
    "                        q_taken = q_values_all[action_taken].item()\n",
    "                    \n",
    "                    # Compute advantage\n",
    "                    advantage = q_taken - baseline\n",
    "                    \n",
    "                    # Policy gradient with COMA advantage + ENTROPY\n",
    "                    obs_i = torch.FloatTensor(traj['local_obs'][t][i]).unsqueeze(0).to(device)\n",
    "                    logits_i = actors[i](obs_i).squeeze(0)\n",
    "                    masked_logits = torch.full_like(logits_i, -1e9)\n",
    "                    masked_logits[feasible_actions] = logits_i[feasible_actions]\n",
    "                    policy_probs = F.softmax(masked_logits, dim=-1)\n",
    "                    \n",
    "                    log_prob = torch.log(policy_probs[action_taken] + 1e-8)\n",
    "                    \n",
    "                    # ENTROPY REGULARIZATION\n",
    "                    entropy = -torch.sum(policy_probs * torch.log(policy_probs + 1e-8))\n",
    "                    \n",
    "                    # Combined loss: Policy gradient + Entropy regularization\n",
    "                    actor_loss_sample = -advantage * log_prob - entropy_coef * entropy\n",
    "                    \n",
    "                    agent_actor_loss += actor_loss_sample\n",
    "                    valid_samples += 1\n",
    "                \n",
    "                if valid_samples > 0:\n",
    "                    agent_actor_loss = agent_actor_loss / valid_samples\n",
    "                    total_actor_loss += agent_actor_loss\n",
    "                    \n",
    "                    # Update actor i\n",
    "                    actor_optimizers[i].zero_grad()\n",
    "                    agent_actor_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(actors[i].parameters(), max_norm=10.0)\n",
    "                    actor_optimizers[i].step()\n",
    "            \n",
    "            episode_actor_loss = (total_actor_loss / M).item() if total_actor_loss != 0 else 0.0\n",
    "        \n",
    "        # Store losses\n",
    "        critic_losses.append(episode_critic_loss)\n",
    "        actor_losses.append(episode_actor_loss)\n",
    "        \n",
    "        print(f\"Episode {episode+1:4d}/{episodes} | Total Reward: {total_reward:8.3f} | \"\n",
    "              f\"Critic Loss: {episode_critic_loss:8.4f} | Actor Loss: {episode_actor_loss:8.4f}\")\n",
    "\n",
    "    return actors, critic, all_returns, critic_losses, actor_losses, detailed_episode_logs\n",
    "\n",
    "\n",
    "\n",
    "def save_trained_models(actors, critic, returns=None, save_dir=\"saved_models\"):\n",
    "    \"\"\"\n",
    "    Save your trained COMA models\n",
    "    \"\"\"\n",
    "    # Create timestamped directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = os.path.join(save_dir, f\"coma_model_{timestamp}\")\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    \n",
    "    # Save each actor\n",
    "    for i, actor in enumerate(actors):\n",
    "        torch.save(actor.state_dict(), os.path.join(model_path, f\"actor_{i}.pth\"))\n",
    "    \n",
    "    # Save critic\n",
    "    torch.save(critic.state_dict(), os.path.join(model_path, \"critic.pth\"))\n",
    "    \n",
    "    # Save training results if provided\n",
    "    if returns is not None:\n",
    "        with open(os.path.join(model_path, \"training_results.pkl\"), 'wb') as f:\n",
    "            pickle.dump({'returns': returns}, f)\n",
    "    \n",
    "    print(f\" Models saved to: {model_path}\")\n",
    "    return model_path\n",
    "\n",
    "def load_trained_models(model_path, env):\n",
    "    \"\"\"\n",
    "    Load your saved models\n",
    "    \"\"\"\n",
    "    # Recreate network architectures\n",
    "    state_dim = len(env.reset())\n",
    "    M, H = env.M, env.H\n",
    "    action_dim = 3 + H\n",
    "    obs_dim = 4 + H\n",
    "    \n",
    "    # Initialize networks\n",
    "    actors = [Actor(obs_dim, action_dim, hidden_dim=128).to(device) for _ in range(M)]\n",
    "    critic = COMACritic(state_dim, M, action_dim, hidden_dim=256).to(device)\n",
    "    \n",
    "    # Load weights\n",
    "    for i, actor in enumerate(actors):\n",
    "        actor.load_state_dict(torch.load(os.path.join(model_path, f\"actor_{i}.pth\"), map_location=device))\n",
    "        actor.eval()\n",
    "    \n",
    "    critic.load_state_dict(torch.load(os.path.join(model_path, \"critic.pth\"), map_location=device))\n",
    "    critic.eval()\n",
    "    \n",
    "    print(f\" Models loaded from: {model_path}\")\n",
    "    return actors, critic"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
